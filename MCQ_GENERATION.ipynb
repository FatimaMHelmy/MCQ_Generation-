{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MCQ_GENERATION.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq9tl8jl25qG"
      },
      "source": [
        "# #Get the required libraries\n",
        "\n",
        "import pandas as pd \n",
        "import re #regular expression  \n",
        "import string \n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_n7DUnW3CCr"
      },
      "source": [
        "## import source of data \n",
        "import pandas as pd  \n",
        "\n",
        "path = \"/content/T5_TEXT.txt\"\n",
        "''' to read text file '''\n",
        "with open(path, 'rb') as f:\n",
        "    python_string = f.read()\n",
        "python_string = python_string.decode(\"utf-8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA0DJYFt32lz"
      },
      "source": [
        "## PreProcessing [link text](https://)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIkcyBQ73F0f"
      },
      "source": [
        "# DATA CLEANING \n",
        "import string \n",
        "def clean_data(original_text):\n",
        "    modified_text=original_text.lower()\n",
        "    modified_text=re.sub ('[\\t\\n\\r\\f\\v]','',modified_text)\n",
        "    modified_text=re.sub ('[()[\\]{}]','',modified_text)\n",
        "    modified_text=re.sub('\\[\\d+\\]','',modified_text)\n",
        "    ''' delat arabic words '''\n",
        "    modified_text = re.sub('[^a-zA-z0-9@-_.% \"]', '',modified_text)\n",
        "    return modified_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f2izgRI3R41"
      },
      "source": [
        "OUTPUT=clean_data(python_string)\n",
        "OUTPUT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it-ZH6qR3vH2"
      },
      "source": [
        "### KeyWordsExtraction using YAKE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soCDwr4D4QpR"
      },
      "source": [
        "!pip install pywsd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_p-ThbX4gHe"
      },
      "source": [
        "! pip install yake"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgewWNoy3jWv"
      },
      "source": [
        "import yake\n",
        "keywords_FINAL=[]\n",
        "kw_extractor = yake.KeywordExtractor()\n",
        "#text = \"\"\"spaCy is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython. The library is published under the MIT license and its main developers are Matthew Honnibal and Ines Montani, the founders of the software company Explosion.\"\"\"\n",
        "language = \"en\"\n",
        "max_ngram_size = 3\n",
        "deduplication_threshold = 0.9\n",
        "numOfKeywords = 20\n",
        "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
        "keywords = custom_kw_extractor.extract_keywords(OUTPUT)\n",
        "for kw in keywords:\n",
        "    keywords_FINAL.append(kw[0])\n",
        "print(f'keywords,{keywords_FINAL}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AwUATq1WOYV"
      },
      "source": [
        "keywords_FINAL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnspct8n4yb6"
      },
      "source": [
        "## sentence Mapping \n",
        "(https://colab.research.google.com/github/Raja-mishra1/Mcq-s-with-Bert-summariser/blob/master/GenerateMCQwithBERT_summarizer_wordnet_and_conceptnet_.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81NWGAME4xmO"
      },
      "source": [
        "!pip install -U nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwesiLnR7b9H"
      },
      "source": [
        "! pip install flashtext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZsJtqeW7O1G"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from flashtext import KeywordProcessor\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "def tokenize_sentences(text):\n",
        "    sentences = [sent_tokenize(text)]\n",
        "    sentences = [y for x in sentences for y in x]\n",
        "    \n",
        "    # Remove any short sentences less than 20 letters.\n",
        "    #sentences = [sentence.strip() for sentence in sentences if len(sentence) > 20]\n",
        "    return sentences\n",
        "\n",
        "def get_sentences_for_keyword(keywords, sentences):\n",
        "    keyword_processor = KeywordProcessor()\n",
        "    keyword_sentences = {}\n",
        "    for word in keywords:\n",
        "        keyword_sentences[word] = []\n",
        "        keyword_processor.add_keyword(word)\n",
        "    for sentence in sentences:\n",
        "        keywords_found = keyword_processor.extract_keywords(sentence)\n",
        "        for key in keywords_found:\n",
        "            keyword_sentences[key].append(sentence)\n",
        "\n",
        "    for key in keyword_sentences.keys():\n",
        "        values = keyword_sentences[key]\n",
        "        values = sorted(values, key=len, reverse=True)\n",
        "        keyword_sentences[key] = values\n",
        "    return keyword_sentences\n",
        "\n",
        "sentences = tokenize_sentences(OUTPUT)\n",
        "keyword_sentence_mapping = get_sentences_for_keyword(keyphrases, sentences)\n",
        "        \n",
        "print (f'sentences{sentences},/n keyword_sentence_mapping {keyword_sentence_mapping}')\n",
        "len(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkXnmG21WgRm"
      },
      "source": [
        "len(keyphrases)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50eptmXpWnOO"
      },
      "source": [
        "keyword_sentence_mapping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGzFG_aoX_DN"
      },
      "source": [
        "# delet non valuable keys\n",
        "for k in keyword_sentence_mapping.keys():\n",
        "  if keyword_sentence_mapping[k]==[]:\n",
        "    #del(keyword_sentence_mapping[k])\n",
        "    print(k)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYzOYpG9awJY"
      },
      "source": [
        "del(keyword_sentence_mapping[\"buy products based\"\n",
        "])    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVJydsBYZlDH"
      },
      "source": [
        "keyword_sentence_mapping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tducIJ0dJ7mF"
      },
      "source": [
        "## delet duplicated sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixkDFWIalB1B"
      },
      "source": [
        "for key in keyword_sentence_mapping.keys():\n",
        "  values=keyword_sentence_mapping[key] \n",
        "  df = pd.DataFrame({'values':values})\n",
        "  df.drop_duplicates(inplace = True)\n",
        "  keyword_sentence_mapping[key] = df['values'].tolist() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKr1IQpmlmjI"
      },
      "source": [
        "len(keyword_sentence_mapping)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cUu2mmhbdTW"
      },
      "source": [
        "## creat Questions using T5\n",
        "https://drive.google.com/drive/folders/1aDic552-3JkGD8-v298tPMoaIvByLPJ4?usp=sharing\n",
        "\n",
        "> Indented block\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLuZPBy6bdH0"
      },
      "source": [
        "!pip install --quiet git+https://github.com/huggingface/transformers.git@5c00918681d6b4027701eb46cea8f795da0d4064\n",
        "!pip install --quiet sentencepiece==0.1.95"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUSmizSbbD0b"
      },
      "source": [
        "!pip install --quiet ipython-autotime\n",
        "%load_ext autotime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLnVbkdzx0GS"
      },
      "source": [
        "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
        "\n",
        "#T5 model size on disk ~ 900 MB\n",
        "question_model = T5ForConditionalGeneration.from_pretrained('ramsrigouthamg/t5_squad_v1')\n",
        "question_tokenizer = T5Tokenizer.from_pretrained('ramsrigouthamg/t5_squad_v1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1wqIwwud_1D"
      },
      "source": [
        "def get_question(sentence,answer,mdl,tknizer):\n",
        "  text = \"context: {} answer: {}\".format(sentence,answer)\n",
        "  #print (text)\n",
        "  max_len = 256\n",
        "  encoding = tknizer.encode_plus(text,max_length=max_len, pad_to_max_length=False,truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "  input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "\n",
        "  outs = mdl.generate(input_ids=input_ids,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  early_stopping=True,\n",
        "                                  num_beams=5,\n",
        "                                  num_return_sequences=1,\n",
        "                                  no_repeat_ngram_size=2,\n",
        "                                  max_length=300)\n",
        "\n",
        "\n",
        "  dec = [tknizer.decode(ids,skip_special_tokens=True) for ids in outs]\n",
        "\n",
        "\n",
        "  Question = dec[0].replace(\"question:\",\"\")\n",
        "  Question= Question.strip()\n",
        "  return Question\n",
        "\n",
        "\n",
        "#context = \"consumers buy products in smaller volumes suitable for personal use.\"\n",
        "#answer = \"business\"\n",
        "\n",
        "#ques = get_question(context,answer,question_model,question_tokenizer)\n",
        "#print (\"question: \",ques)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XZmlgGinc6T"
      },
      "source": [
        "QUESTIONS=[]\n",
        "CONTEXT_LIST=[]\n",
        "for k in keyword_sentence_mapping.keys():\n",
        "  answer=k\n",
        "  value=keyword_sentence_mapping[k]\n",
        "  \n",
        "  context = value[0]\n",
        "  ques = get_question(context,answer,question_model,question_tokenizer)\n",
        "  QUESTIONS.append(ques)\n",
        "  CONTEXT_LIST.append(context)\n",
        "  #print (\"question: \",ques)\n",
        "  #print(answer)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-QubpGGHkvm"
      },
      "source": [
        "\n",
        "# x=list(keyword_sentence_mapping.values())\n",
        "# x\n",
        "QUESTIONS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxAux2KDJlSO"
      },
      "source": [
        "len( QUESTIONS)\n",
        "# CONTEXT_LIST\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDm5xnMssrSk"
      },
      "source": [
        "##ALLENNLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ia1TW6Ess40"
      },
      "source": [
        "pip install allennlp==2.1.0 allennlp-models==2.1.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brkx5Ql5uwcU"
      },
      "source": [
        "!echo '{\"passage\": \"The Matrix is a 1999 science fiction action film written and directed by The Wachowskis, starring Keanu Reeves, Laurence Fishburne, Carrie-Anne Moss, Hugo Weaving, and Joe Pantoliano.\", \"question\": \"Who stars in The Matrix?\"}' | \\\n",
        "    allennlp predict https://storage.googleapis.com/allennlp-public-models/bidaf-elmo.2021-02-11.tar.gz -"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbM6dTJCs8F0"
      },
      "source": [
        "from allennlp.predictors.predictor import Predictor\n",
        "import allennlp_models.rc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIPka7XMzTvb"
      },
      "source": [
        "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/bidaf-elmo.2021-02-11.tar.gz\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8gJT5Sg0DXe"
      },
      "source": [
        "for n in range(len(CONTEXT_LIST)):\n",
        "\n",
        "  pre=predictor.predict(\n",
        "  passage=CONTEXT_LIST[n],question=QUESTIONS[n])\n",
        "  for k in keyword_sentence_mapping.keys():\n",
        "    print(\"original answer:\",k)\n",
        "    print(\"predicted answer:\",pre['best_span_str'])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgKuOU_YWRkl"
      },
      "source": [
        "## Another keyWords Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRg8FKUcWVDN"
      },
      "source": [
        "## install packages\n",
        "!pip install trafilatura\n",
        "!pip install summa\n",
        "!pip install git+https://github.com/smirnov-am/pytopicrank.git#egg=pytopicrank\n",
        "!pip install git+https://github.com/LIAAD/yake\n",
        "!pip install keyBERT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_jf_sOUWrQo"
      },
      "source": [
        "\n",
        "Keywords Extraction with BERT\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_31d_hJF96-"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdEAZpNpWw2f"
      },
      "source": [
        "#from transformers.modeling_bert import BertModel, BertForMaskedLM\n",
        "\n",
        "from keybert import KeyBERT\n",
        "kw_extractor = KeyBERT('distilbert-base-nli-mean-tokens')\n",
        "for j in range(len(keyword_sentence_mapping)):\n",
        "   \n",
        "    keywords = kw_extractor.extract_keywords(keyword_sentence_mapping[j], keyphrase_length=1, stop_words='english')\n",
        "    for l in keyword_sentence_mapping.keys():\n",
        "     #print(\"keywords_bert:\",keywords)\n",
        "      print(\"yake_keywords_bert:\",l)\n",
        "      print(\"keywords_bert\", str(j+1), \"\\n\", keywords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKaAIfXMGAKW"
      },
      "source": [
        "### PKE\n",
        "https://boudinfl.github.io/pke/build/html/tutorials/input.html\n",
        "\n",
        "## founded that it's the mostaccurate extractor "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaYXYznKXg1w"
      },
      "source": [
        "## source code _repo\n",
        "https://github.com/ramsrigouthamg/pke"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn1LR0j0GCrk"
      },
      "source": [
        "!pip install git+https://github.com/boudinfl/pke.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTHXqVeOGLMR"
      },
      "source": [
        "  !python -m nltk.downloader stopwords\n",
        "  !python -m nltk.downloader universal_tagset\n",
        "  !python -m spacy download en # download the english model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5M7_OZaGOG3"
      },
      "source": [
        "import pke\n",
        "\n",
        "# initialize keyphrase extraction model, here TopicRank\n",
        "extractor = pke.unsupervised.TopicRank()\n",
        "\n",
        "# load the content of the document, here document is expected to be in raw\n",
        "# format (i.e. a simple text file) and preprocessing is carried out using spacy\n",
        "extractor.load_document(input='T5_TEXT.txt', language='en')\n",
        "\n",
        "# keyphrase candidate selection, in the case of TopicRank: sequences of nouns\n",
        "# and adjectives (i.e. `(Noun|Adj)*`)\n",
        "extractor.candidate_selection()\n",
        "\n",
        "# candidate weighting, in the case of TopicRank: using a random walk algorithm\n",
        "extractor.candidate_weighting()\n",
        "\n",
        "# N-best selection, keyphrases contains the 10 highest scored candidates as\n",
        "# (keyphrase, score) tuples\n",
        "keyphrases = extractor.get_n_best(n=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmBiWArGGmxr"
      },
      "source": [
        "keyphrases = [ seq[0] for seq in keyphrases ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrmZ9MpyGnix"
      },
      "source": [
        "keyphrases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1eNfQf7XNh1"
      },
      "source": [
        "### Distractor-Generation-RACE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kYWkEA9GyGA"
      },
      "source": [
        "# download glive \n",
        "!wget http://nlp.stanford.edu/data/glove.840B.300d.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLOgZXMZk31V"
      },
      "source": [
        "!unzip glove*.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxb1iQN2oorO"
      },
      "source": [
        "!wget https://github.com/Yifan-Gao/Distractor-Generation-RACE/blob/master/data/data.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_ZDsPkipzKI"
      },
      "source": [
        "!unzip data.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnZSekrLp5og"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}